from typing import List, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer

from split import read_random_task_document, split_content1, split_content2
from split_match import MatchGenerator
import re


class ColonBasedMatcher:
    def __init__(self):
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    def _split_by_colon(self, text: str) -> List[str]:
        """
        ç²¾å‡†å†’å·æ®µè½åˆ‡åˆ†ç®—æ³•ï¼ˆæ®µé¦–ç¬¬ä¸€ä¸ªå†’å·ä¸åˆ†å‰²ï¼‰
        è§„åˆ™ï¼š
        1. ç¬¬ä¸€ä¸ªå†’å·è¡Œä¸è§¦å‘åˆ†å‰²ï¼Œåç»­å†’å·è¡Œè§¦å‘æ–°æ®µè½
        2. æ¯ä¸ªå†’å·è¡Œçš„å‰ä¸€è¡Œä½œä¸ºæ ‡é¢˜è¡Œ
        3. ç©ºå†’å·è¡Œåˆå¹¶åˆ°å‰ä¸€æ®µè½
        """
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        if not lines:
            return []

        # è¯†åˆ«æ‰€æœ‰å†’å·è¡Œï¼ˆåŒ…å«ç©ºå†’å·ï¼‰
        colon_indices = [i for i, line in enumerate(lines) if re.search(r'[:ï¼š]', line)]
        colon_indices.insert(0, 0)

        # æ²¡æœ‰å†’å·è¡Œæˆ–åªæœ‰ä¸€ä¸ªå†’å·è¡Œæ—¶è¿”å›å…¨æ–‡
        if len(colon_indices) < 2:
            return ["\n".join(lines)]

        # ä»ç¬¬äºŒä¸ªå†’å·å¼€å§‹å¤„ç†ï¼ˆè·³è¿‡ç¬¬ä¸€ä¸ªï¼‰
        split_points = colon_indices[1:]

        paragraphs = []

        current = 0

        # å¤„ç†åç»­æ®µè½
        for i in range(0, len(split_points)):
            next_point = split_points[i + 1] if i < len(split_points) - 1 else len(lines)

            if next_point == len(lines):
                paragraphs.append("\n".join(lines[current:next_point]))
                break

            # å¤„ç†ç©ºå†’å·è¡Œ
            if lines[next_point].startswith('ï¼š') or lines[next_point].startswith(':'):
                paragraphs.append("\n".join(lines[current:next_point - 1]))
                current = next_point - 1
            else:
                paragraphs.append("\n".join(lines[current:next_point]))
                current = next_point

        if len(paragraphs) >= 3 and paragraphs[1].strip().endswith((":", "ï¼š")):
            merged = paragraphs[1] + '\n' + paragraphs[2]
            paragraphs = [paragraphs[0], merged] + paragraphs[3:]

        return paragraphs

    def enhanced_pair_match(self, content1_top_paras: List[str], content2_para: str) -> List[Tuple[str, List[dict]]]:
        """
        æ”¹è¿›åçš„åŒ¹é…æµç¨‹ï¼ˆä¿æŒåŸæœ‰æ¥å£ï¼‰
        """
        # 1. æŒ‰å†’å·åˆ‡åˆ†content2æ®µè½
        content2_segments = self._split_by_colon(content2_para)

        # 2. ç”Ÿæˆcontent1çš„ç»„åˆï¼ˆä¿æŒåŸæœ‰5è¡Œçª—å£é€»è¾‘ï¼‰
        # åˆå§‹åŒ–content1_pairså’Œpair_originåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å¤„ç†åçš„æ®µè½å’ŒåŸå§‹ç´¢å¼•ä¿¡æ¯
        content1_pairs = []
        pair_origin = []

        # éå†content1_top_parasä¸­çš„æ¯ä¸ªæ®µè½åŠå…¶ç´¢å¼•
        for para_idx, para in enumerate(content1_top_paras):
            # å°†æ®µè½æŒ‰è¡Œåˆ†å‰²ï¼Œå»é™¤æ¯è¡Œä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦ï¼Œå¹¶è¿‡æ»¤æ‰ç©ºè¡Œ
            lines = [line.strip() for line in para.split('\n') if line.strip()]

            # ç”Ÿæˆ5è¡Œçª—å£ï¼ˆæ­¥é•¿3ï¼‰ï¼Œå³æ¯æ¬¡å–5è¡Œä½œä¸ºä¸€ä¸ªå—ï¼Œæ¯3è¡Œç§»åŠ¨ä¸€æ¬¡çª—å£
            # è¿™æ ·åšå¯ä»¥å‡å°‘æ•°æ®é‡ï¼ŒåŒæ—¶ä¿æŒæ®µè½çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            chunks = ['\n'.join(lines[i:i + 5]) for i in range(0, len(lines), 3) if i + 5 <= len(lines)]

            if not chunks:
                # fallbackï¼šå¦‚æœä¸€ä¸ªæ®µè½éƒ½æ²¡æ‹†å‡ºchunkï¼Œè‡³å°‘æŠŠå®ƒè‡ªå·±å½“ä¸€ä¸ªchunk
                chunks = [para]
                origin_idxs = [para_idx]
            else:
                origin_idxs = [para_idx] * len(chunks)

            # å°†ç”Ÿæˆçš„å—æ·»åŠ åˆ°content1_pairsåˆ—è¡¨ä¸­
            content1_pairs.extend(chunks)

            # è®°å½•æ¯ä¸ªå—æ¥æºäºå“ªä¸ªæ®µè½ï¼Œä¾¿äºåç»­å¤„ç†
            pair_origin.extend(origin_idxs)

        # 3. æ‰¹é‡ç¼–ç 
        content1_emb = self.model.encode(content1_pairs, convert_to_tensor=True)
        content2_emb = self.model.encode(content2_segments, convert_to_tensor=True)

        # 4. ç›¸ä¼¼åº¦è®¡ç®—ä¸åŠ¨æ€å¯¹é½
        sim_matrix = np.inner(content2_emb.cpu(), content1_emb.cpu())

        # 5. å¸¦çº¦æŸçš„åŒ¹é…ï¼ˆå‰å‘çª—å£=3ï¼‰
        results = []
        prev_match_idx = 0
        for seg_idx in range(len(content2_segments)):
            # é™åˆ¶æœç´¢èŒƒå›´
            start = max(0, prev_match_idx - 1)
            end = min(sim_matrix.shape[1], prev_match_idx + 4)
            best_idx = np.argmax(sim_matrix[seg_idx, start:end]) + start

            matches = []
            if best_idx < len(pair_origin):
                origin_idx = pair_origin[best_idx]
                matches.append({
                    "content1_pair": content1_pairs[best_idx],
                    "similarity": float(sim_matrix[seg_idx][best_idx]),
                    "source_para": content1_top_paras[origin_idx],
                    "source_idx": origin_idx
                })
                prev_match_idx = best_idx

            results.append((
                content2_segments[seg_idx],
                sorted(matches, key=lambda x: x['similarity'], reverse=True)[:2]
            ))

        return results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–åŒ¹é…å™¨
    matcher = ColonBasedMatcher()  # è¡¥å…¨æ‹¬å·åˆ›å»ºå®ä¾‹

    # è¯»å–å¹¶åˆ†å‰²æ–‡æ¡£
    for i in range(10):
        content1, content2 = read_random_task_document("generate_text/doc_binary/0")
        content1_paras = split_content1(content1)
        content2_paras = split_content2(content2)

        # ç”Ÿæˆæ®µè½çº§åŒ¹é…çŸ©é˜µ
        generator = MatchGenerator()
        sim_matrix = generator.build_matrix(content1_paras, content2_paras)
        para_matches = generator.get_top_matches(sim_matrix, content1_paras, content2_paras, top_k=2)

        # éå†æ¯ä¸ªcontent2æ®µè½è¿›è¡Œä¸¤è¡Œç»„åˆåŒ¹é…
        for para_match in para_matches:
            content2_para = para_match['source_para']
            content1_top_paras = [match['target_para'] for match in para_match["matches"]]

            print("\n" + "=" * 60)
            print(f"â–ŒContent2å½“å‰æ®µè½ï¼š\n{content2_para}")
            print("=" * 60)

            # æ‰§è¡Œä¸¤è¡Œç»„åˆåŒ¹é…
            pair_matches = matcher.enhanced_pair_match(content1_top_paras, content2_para)

            if not pair_matches:
                print("âš  æœªæ‰¾åˆ°æœ‰æ•ˆåŒ¹é…ç»„åˆ")
                continue

            # æ‰“å°åŒ¹é…ç»“æœ
            for pair_idx, (content2_pair, match_list) in enumerate(pair_matches, 1):
                print(f"\nâ–Œç»„åˆ{pair_idx}: {content2_pair.replace('[SEP]', ' â” ')}")

                for match_idx, match in enumerate(match_list, 1):
                    print(f"  ğŸ…Top-{match_idx} åŒ¹é…:")
                    print(f"  ç›¸ä¼¼åº¦: {match['similarity']:.2%}")
                    # print(f"  æ¥æºæ®µè½: {match['source_para']}")
                    print(f"  ã€åŒ¹é…å†…å®¹ã€‘: {match['content1_pair'].replace('[SEP]', ' â‡¨ ')}")
                    print("-" * 60)